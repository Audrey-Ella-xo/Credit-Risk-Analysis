# Loan Default Prediction in P2P Lending  
**Integrating Explainable AI into Ensemble Machine Learning Models for Enhanced Credit Risk Prediction**

This project builds and evaluates a set of machine learning models to predict **loan default risk** in a peer-to-peer (P2P) lending context.  
It focuses on:

- Reducing **false negatives** (i.e. missing risky borrowers),
- Evaluating **business cost impact** of misclassifications, and
- Improving transparency using **Explainable AI (XAI)** with **LIME**.

All of the work is implemented in the Jupyter notebook:

> `P2P_Bondara.ipynb`

---

## üìå 1. Project Overview

P2P lending platforms need to assess whether a borrower will **default** or **repay**.  
This notebook walks through the full workflow:

1. Data loading and cleaning  
2. Feature engineering & selection  
3. Model training and hyperparameter tuning  
4. Threshold tuning to minimise **false negatives**  
5. Business cost analysis of prediction errors  
6. Model explainability using **LIME** for local interpretations

The final output is a tuned classification model optimised for **recall**, plus XAI explanations that show **why** the model classifies a specific borrower as ‚Äúdefault‚Äù or ‚Äúno default‚Äù.

---

## üíæ 2. Data

The dataset represents historical loan records from a P2P lending platform.  
Each row corresponds to a loan and contains:

- **Borrower information** (e.g. income-related fields, employment, etc.)
- **Loan characteristics** (e.g. amount, monthly payment, maturity)
- **Account / behavioural information**
- **Outcome variable** indicating default / non-default

Key steps applied in the notebook:

- Removal of **obsolete / leakage features**, including:
  - IDs and purely technical columns (e.g. `LoanId`, `LoanNumber`)
  - Several income breakdown fields
- Removal of multiple **date-related columns** that are not useful for modelling (e.g. `ListedOnUTC`, `BiddingStartedOn`, `LoanApplicationStartedDate`, etc.)
- Creation of a **binary target variable**:
  - Original status labels like *Repaid*, *Current*, *Late* and a `DefaultDate` field are used.
  - Status + default date information are combined into a single target column: `LoanStatus` (default vs no default).

> üìå You will need the loan dataset (e.g. `LoanData.csv`) stored in your own environment.  
> The notebook expects it to be loaded from Google Drive ‚Äì update the path if running locally.

---

## üßπ 3. Data Preparation

The data preparation pipeline in the notebook includes:

1. **Missing Value Handling**  
   - Identification of missing values per column.  
   - Appropriate imputation or column removal depending on missingness and relevance.

2. **Outlier Handling**  
   - Use of visualisations (boxplots, histograms) to inspect skewness and extreme values.  
   - Outlier treatment using distribution-based rules (e.g. IQR) for key numeric features such as amount and monthly payment.

3. **Type & Category Cleanup**  
   - Ensuring numeric vs categorical types are correct.
   - Converting selected integer/float columns into categorical where appropriate.

4. **Encoding Categorical Variables**  
   - **Label Encoding** is applied:
     - Target (`y = LoanStatus`) is encoded to 0/1.
     - All categorical predictors in `X` are iterated and label-encoded.

5. **Feature Scaling**  
   - **StandardScaler** is used to scale `X` after encoding.

6. **Feature Selection**  
   - `SelectKBest` with `mutual_info_classif` is applied to select the **top 15 features** most relevant to the default prediction target.

7. **Train‚ÄìTest Split**  
   - Data is split into training and test sets for unbiased evaluation.

---

## ü§ñ 4. Models & Algorithms

The notebook trains and compares several classification models:

- **Logistic Regression**
- **Linear Support Vector Machine (LinearSVC)**
- **Random Forest Classifier**
- **XGBoost Classifier**

A common training pattern is used:

- Hyperparameters defined in a dictionary for each model.
- **GridSearchCV** with cross-validation to:
  - Tune hyperparameters,
  - Evaluate model performance using multiple metrics.

For each model, the notebook collects:

- Best hyperparameters,
- Validation scores: **precision**, **recall**, **ROC AUC**, and **F1-score**.

A comparison plot is generated to visualise how each model performs across these metrics.

---

## üéØ 5. Model Selection & Threshold Tuning

Instead of simply picking the model with the best overall F1-score, the project focuses on **minimising False Negatives (FN)** ‚Äî i.e. borrowers who default but are incorrectly predicted as safe.

Steps:

1. **Model Selection**  
   - From the GridSearch results, the **best model is selected based on Recall**, not just accuracy or F1.

2. **Threshold Tuning**  
   - The chosen model outputs class **probabilities** for the ‚Äúdefault‚Äù class.
   - A range of thresholds (e.g. 0.1 ‚Üí 0.5) is tested.
   - For each threshold:
     - Predictions are made,
     - Recall is calculated.
   - The threshold that yields the **highest recall** is selected as the **best operating point**.

This is crucial for credit risk, where missing a risky borrower (FN) is often far more costly than incorrectly flagging a good one (FP).

---

## üìä 6. Evaluation & Metrics

The notebook calculates and visualises several metrics:

- **Confusion Matrix**
- **Accuracy**
- **Precision**
- **Recall**
- **F1-score**
- **ROC Curve & AUC**

The confusion matrix is particularly important, as it underpins the **business cost analysis**.

---

## üí∂ 7. Business Cost Analysis

The notebook explicitly quantifies the financial impact of wrong predictions:

- Assumed costs:
  - `FN_cost = 5000` ‚Üí Cost of granting a loan to a borrower who actually defaults (False Negative).
  - `FP_cost = 500`  ‚Üí Cost of incorrectly rejecting or flagging a good borrower (False Positive).

Using the confusion matrix:

- `fn_count = cm[1, 0]`
- `fp_count = cm[0, 1]`

The total cost is computed as:

```text
Total Misclassification Cost
= FN_cost √ó FN_count + FP_cost √ó FP_count

A bar chart then visualises:
	‚Ä¢	Total cost from False Negatives vs
	‚Ä¢	Total cost from False Positives

This ties the model performance back to real business impact.

‚∏ª

üß† 8. Explainable AI with LIME

To improve transparency, the project uses LIME (Local Interpretable Model-Agnostic Explanations):
	‚Ä¢	A LimeTabularExplainer is created using the scaled training data and the original feature names.
	‚Ä¢	A specific test instance (e.g. the first row of X_test) is selected.
	‚Ä¢	LIME generates:
	‚Ä¢	A local explanation showing the top features that pushed the model towards ‚ÄúDefault‚Äù or ‚ÄúNo Default‚Äù.
	‚Ä¢	These are displayed in-notebook using exp.show_in_notebook(...).

This allows stakeholders to understand why the model predicted that a given borrower is risky, which is critical for:
	‚Ä¢	Regulatory compliance,
	‚Ä¢	Internal risk governance,
	‚Ä¢	Customer communication.

‚∏ª

üìÅ 9. Project Structure

A typical repository layout for this project could look like:

.
‚îú‚îÄ‚îÄ P2P_Bondara.ipynb      # Main notebook with full pipeline
‚îú‚îÄ‚îÄ LoanData.csv           # (Not included) Raw loan dataset ‚Äì add your own
‚îú‚îÄ‚îÄ README.md              # Project documentation (this file)
‚îî‚îÄ‚îÄ requirements.txt       # Python dependencies (optional)

Note: The dataset is not bundled here. You must provide your own loan dataset and update the path in the notebook.

‚∏ª

‚öôÔ∏è 10. How to Run the Notebook

Option A ‚Äì Google Colab (recommended)
	1.	Upload P2P_Bondara.ipynb to Google Colab.
	2.	Upload the dataset (e.g. LoanData.csv) to your Google Drive.
	3.	In the notebook:
	‚Ä¢	The following lines mount Google Drive:

from google.colab import drive
drive.mount('/content/drive')


	‚Ä¢	Update the pd.read_csv(...) path to point to your dataset location in Drive.

	4.	Run all cells, top to bottom.

Option B ‚Äì Local Jupyter
	1.	Install dependencies (example):

pip install pandas numpy matplotlib seaborn scikit-learn xgboost lime


	2.	Place LoanData.csv in a local folder and update the file path in the read_csv call.
	3.	Launch Jupyter:

jupyter notebook


	4.	Open P2P_Bondara.ipynb and run all cells.

‚∏ª

üì¶ 11. Dependencies

Core Python libraries used:
	‚Ä¢	pandas
	‚Ä¢	numpy
	‚Ä¢	matplotlib
	‚Ä¢	seaborn
	‚Ä¢	scikit-learn
	‚Ä¢	LogisticRegression, LinearSVC, RandomForestClassifier
	‚Ä¢	train_test_split, KFold, GridSearchCV, ShuffleSplit
	‚Ä¢	StandardScaler, SelectKBest, mutual_info_classif
	‚Ä¢	Metrics: accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
	‚Ä¢	xgboost (XGBClassifier)
	‚Ä¢	lime (LimeTabularExplainer)

You can optionally create a requirements.txt with these packages for reproducibility.

‚∏ª

üöÄ 12. Possible Extensions

Some ideas to take this further:
	‚Ä¢	Try additional models or stacked ensembles.
	‚Ä¢	Apply class weighting or cost-sensitive learning directly in the algorithms.
	‚Ä¢	Use SMOTE or other resampling methods if the dataset is highly imbalanced.
	‚Ä¢	Extend XAI:
	‚Ä¢	Global explanations (e.g. feature importance, SHAP values),
	‚Ä¢	Compare human-interpretable rules to model outputs.
	‚Ä¢	Deploy the best model as an API or simple web dashboard for risk analysts.

‚∏ª

üôè 13. Acknowledgements

This project builds on public P2P lending datasets and standard Python ML/XAI libraries.
Special thanks to the open-source community behind scikit-learn, XGBoost, and LIME, and to the creators of the underlying loan dataset.
